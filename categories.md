Advance Organizers (Ausubel, 1960, 1968). Providing an abstract organizational framework before detailed information significantly improves comprehension and retention. Ausubel's original studies and subsequent meta-analyses (Luiten et al., 1980; Stone, 1983) show reliable effects. The LLM analogue would be giving the model a structural outline or schema before providing the detailed context it needs to reason over — e.g., "This problem involves three constraints: resource allocation, time ordering, and compatibility. Here are the details: [CONTEXT]." Nobody has tested whether providing such scaffolding before context improves LLM accuracy versus just dumping context directly. This is distinct from system prompts or role-setting, which tell the model how to behave, not how to organize incoming information.
The Protégé Effect / Learning-by-Teaching (Fiorella & Mayer, 2013; Roscoe & Chi, 2007). People understand material better when they prepare to teach it or actually explain it to someone else. The mechanism involves deeper processing, gap detection, and self-monitoring. The LLM analogue: "Explain this concept to a novice, then use that understanding to answer this expert-level question." While "explain like I'm five" is used colloquially, nobody has formally tested whether generating a simplified explanation first improves subsequent complex reasoning on the same material. The key distinction from self-explanation is that teaching targets an audience (forcing simplification and completeness checking) while self-explanation is internal comprehension monitoring.
Productive Failure (Kapur, 2008, 2014). Related to but distinct from the pretesting effect. Kapur showed that students who struggle with complex problems before receiving instruction significantly outperform students who receive instruction first, even though their initial solutions are wrong. The critical difference from pretesting is that productive failure involves complex, open-ended problem-solving (not simple factual retrieval), and the mechanism involves generating multiple diverse solution strategies. The LLM analogue would be asking the model to generate 3–4 different possible approaches to a hard problem before providing it with the correct method or relevant context. Kapur's meta-analytic work (Kapur, 2016) shows robust effects across STEM domains.
Encoding Variability (Estes, 1955; Bjork, 1975; Smith et al., 1978). Studying the same material in different contexts or formats improves retrieval. The classic study by Smith et al. (1978) showed that students who studied in two different rooms outperformed those who studied twice in the same room. For LLMs, this could mean presenting the same information in multiple formats within a single prompt — e.g., providing a table AND a prose description AND a set of key-value pairs, rather than just one format. The "lost in the middle" literature (Liu et al., 2024) studies position effects but not format variability effects on the same information.
Analogical Reasoning / Structure Mapping (Gentner, 1983; Gick & Holyoak, 1980, 1983). People solve novel problems more effectively when first presented with an analogous problem from a different domain, especially when the structural correspondence is made explicit. Gick & Holyoak's classic work showed that providing a military analogy dramatically improved solution rates on Duncker's radiation problem — but only when participants were told to use the analogy. For LLMs, the intervention would be: before presenting a hard problem, provide a solved analogy from a different domain and instruct the model to map the structure. This is distinct from few-shot prompting (which provides examples of the same problem type) and from CoT (which doesn't involve cross-domain mapping).
Hypercorrection Effect (Butterfield & Metcalfe, 2001, 2006). High-confidence errors are more easily corrected than low-confidence errors once feedback is provided. This is deeply counterintuitive — you'd expect confident mistakes to be sticky. The mechanism appears to involve surprise-driven attention. For LLMs, the analogue would be asking the model to state its confidence before answering, then providing corrective information and asking again. The prediction: problems where the model initially expresses high confidence but is wrong should show the largest improvement after correction. Nobody has tested this specific confidence→correction→re-answer sequence.
Transfer-Appropriate Processing (Morris, Bransford & Franks, 1977). Memory and performance are best when the cognitive processes used during encoding match the processes required at test. Rhyming study helps with rhyming tests; semantic study helps with meaning tests. For LLMs, this suggests that the type of processing elicited by a prompt should match the type of output required. If the task requires numerical computation, the prompt should elicit numerical processing (not narrative reasoning) during the setup phase. If the task requires creative generation, the setup should elicit associative/divergent processing. Nobody has systematically tested whether matching prompt-elicited processing mode to task requirements improves performance.
The Segmenting Principle (Mayer & Chandler, 2001; Mayer, 2009). Breaking complex continuous information into labeled, manageable segments improves learning significantly — especially for low-knowledge learners dealing with complex material. This is well-supported in the multimedia learning literature. For LLMs, this would mean comparing performance when complex problems are presented as one monolithic block versus explicitly segmented into labeled parts (e.g., "GIVEN:", "CONSTRAINTS:", "GOAL:", "AVAILABLE TOOLS:"). While good prompt engineering intuitively does this, nobody has formally isolated segmentation as an independent variable or tested when it helps versus hurts.
Judgment of Learning / Metacognitive Monitoring (Nelson & Dunlosky, 1991; Koriat, 1997). Asking learners to judge how well they've learned material improves calibration and subsequent performance. The act of monitoring itself enhances processing. For LLMs, this would mean asking the model to rate its confidence or assess the difficulty of a problem before answering. There's emerging work on LLM calibration, but nobody has tested whether asking for a confidence judgment as part of the prompt improves answer accuracy (not just calibration). The prediction from the psychology literature is that it should.
The Expertise Reversal Effect (Kalyuga et al., 2003; Kalyuga, 2007). Instructional techniques that help novices (worked examples, heavy scaffolding) actively hurt experts by creating redundant processing. This has a direct LLM prediction: prompting strategies that boost performance on easy problems may decrease performance on problems the model can already handle well. This predicts an interaction effect between DDP technique and problem difficulty that nobody has tested. It also suggests that adaptive prompting — using more scaffolding for harder problems, less for easy ones — should outperform any fixed strategy.
